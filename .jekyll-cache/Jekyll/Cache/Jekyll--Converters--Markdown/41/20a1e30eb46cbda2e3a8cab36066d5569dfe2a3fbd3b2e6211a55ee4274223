I"A<p>主成分分析，通常作为一种统计学的研究方法，在诸多领域都得到应用。而实际上，主成分分析的精髓则在于线性代数。</p>

<p>通常来看，对于原始数据，在做分析时，我们往往有以下两个诉求：</p>

<ol>
  <li><strong>各个维度的数据没有相关性</strong>。各维度数据没有相关性意味着，根据数据推导出的因果关系不会受到其他维度数据的影响；同时，同一维度代表着这一维度的几乎全部信息，而不受到其他维度的推动。</li>
  <li><strong>尽可能的去掉影响力小的维度</strong>，<strong>而保留重要的为数不多的维度</strong>。主次分明的数据可以更好的被人类用以分析，从而得出主要的结果。</li>
</ol>

<p>那么，主成分分析就是为了解决以上的两个诉求，而产生的数据预处理方法。主成分分析也有其自身的局限性，这一点本文不做讨论。</p>

<h4 id="数学原理">数学原理</h4>

<h5 id="一具体阐释">一、具体阐释</h5>

<p>原始数据通常不能直接作为分析的对象，于是我们需要用到某种方法找出新的数据。这就是主成分(Principal)。</p>

<p>要找出新的主成分，对原始信息降维，实际上是对原始信息中某些相关性高的维度合成一个。也就是说，把各维度的相关性从高到低排列，按某种方法将其中相关性过高的两个维度（或者三个、四个等）合成一个维度（加权平均，或者说线性组合）。</p>

<p>不过这样想，实际上有点多余，我们当然可以直接把所有的维度全部线性组合起来，只需要让某些系数等于零。为了保证之后主成分的大小不会有过大偏差，还会把这些系数的和不超过 1 。</p>

<p>这样就实现了数据的降维。</p>

<p>那么问题就来了：</p>

<ol>
  <li>按哪一种方法针对相关的维度做线性组合？</li>
  <li>如何实现转换过后的数据各维度之间没有任何相关性？</li>
  <li>转换之后该用什么样的标准选取主成分？</li>
</ol>

<p>也就是说，以上的三个问题就是这一数学模型的目标。</p>

<h5 id="二数学模型">二、数学模型</h5>

<h6 id="模型建立">模型建立</h6>

<p>假设原始数据：
<script type="math/tex">% <![CDATA[
X=
 \begin{bmatrix}
  x_1 & x_2 & ... & x_m
 \end{bmatrix}_{n\times m}
 \quad
 其中\;
 x_i
 \space是n维列向量 \quad
 \forall i \in \{1, 2,...,m\} \space
 \sum x_i=0 %]]></script>
而转换之后的数据矩阵长这个样子：
<script type="math/tex">% <![CDATA[
F=
\begin{bmatrix}
f_1 & f_2 & ... & f_m
\end{bmatrix}_{n\times m}
\quad
其中\space 
f_i
\space 是n 维列向量 %]]></script>
那么又假定转换系数矩阵是</p>

<p><script type="math/tex">% <![CDATA[
U=
\begin{bmatrix}
u_1 & u_2 & ... & u_m
\end{bmatrix} _{m \times m}
\quad
其中 \space 
u_i
\space 是m维列向量 \quad
\sum u_i^2 = 1 %]]></script>
那么得到
<script type="math/tex">F=XU \\
其中有：\forall i \in \{1, 2, 3,...,m\} \quad f_i = Xu_i \space 也就是说，每一个主成分都是原始数据各维度的线性组合</script>
那么为了实现我们的目标，我们还需要一个计算维度间相关性的方法：<strong>协方差矩阵</strong>
$$
\begin{align}
X^{\rm T}X&amp;=
\begin{bmatrix}
x_1 &amp; x_2 &amp; … &amp; x_m
\end{bmatrix} ^{\rm T}
<br />
\begin{bmatrix}
x_1 &amp; x_2 &amp; … &amp; x_m
\end{bmatrix}
&amp;=
\begin{bmatrix}
x_1 ^{\rm T} \ x_2 ^{\rm T} \ … \ x_m ^{\rm T}
\end{bmatrix}</p>

<p>\begin{bmatrix}
x_1 &amp; x_2 &amp; … &amp; x_m
\end{bmatrix} <br />
&amp;=
\begin{bmatrix}
x_1 ^{\rm T} x_1 &amp; x_1 ^{\rm T} x_2 &amp; … &amp; x_1 ^{\rm T} x_m <br />
x_2 ^{\rm T} x_1 &amp; x_2 ^{\rm T} x_2 &amp; … &amp; x_2 ^{\rm T} x_m <br />
…&amp;…&amp;…&amp;…<br />
x_m ^{\rm T} x_1 &amp; x_m ^{\rm T} x_2 &amp; … &amp; x_m ^{\rm T} x_m
\end{bmatrix}
\end{align}
<script type="math/tex">而可以发现：</script>
\begin{align}
x_i ^{\rm T} x_i &amp;= Var(x_i)<br />
 x_i^{\rm T} x_j &amp;= x_j ^{\rm T} x_i =Cov(x_i,x_j)
\end{align}
$$
那么，<strong>实际上 $X ^{\rm T} X$  就是针对 $X$ 中各列向量的协方差矩阵</strong>。</p>

<h6 id="模型导出">模型导出</h6>

<p>现在回到我们对于 $F$ 的要求。各维度相关性为零，也就是说，$F$ 协方差矩阵是<strong>对角矩阵</strong>。
<script type="math/tex">% <![CDATA[
\begin{align}
&F^{\rm T}F=(XU)^{\rm T} XU=U^{\rm T}X^{\rm T}X U \\
&Let \; X^{\rm T}X=C,\; F^{\rm T}F = D, \; then \; D=U^{\rm T}CU \\
&C=(U^{\rm T})^{-1}DU^{-1}
\end{align} %]]></script>
$C$ 是实对称矩阵，$D$ 是对角矩阵，而 $U$ 是也是方阵。眼尖的同学已经发现了，只要我们把矩阵 $C$ 进行<strong>谱分解</strong>，就可以得到这样的结果。</p>

<p>线性代数比较好的同学当然也知道：<strong>正交矩阵的逆矩阵等于其转置矩阵</strong>。（这也就是我们在假定中针对U做出如此限定的原因） $U$ 是正交矩阵，也就是 $C$ 的特征矩阵 $P$ 。于是，我们的结论就是 原始数据的协方差矩阵的特征矩阵就是这里的降维系数矩阵。</p>

<h6 id="主成分的选取">主成分的选取</h6>

<p>至此，问题 1 和问题 2 都得到了解决。还剩下了问题 3 。答案很简单，我们是根据 f 的方差大小决定去留的。方差更大的主成分更值得保留。</p>

<p>那么问题又来了，为什么会是这样？更大的方差到底意味着什么？</p>

<p>不妨这样思考：方差如果太小会怎么样？更小的方差意味着这一维度的数据将会更加集中，而信息量将会减少，而方差为零则更加说明，这一主成分几乎不起任何作用。</p>

<p>那么可以根据主成分的协方差矩阵，在矩阵 $F$ 选取方差较大的<strong>一列</strong>作为，主成分分析的结果。保留的维数根据分析的需求而定，当然也可以随意指定。</p>

<h6 id="基变换和线性空间">基变换和线性空间</h6>

<p>从线性代数的角度来说，生成主成分实际上是一次<strong>基变换</strong>。我们可以把现有主成分称为原始数据在新的基上的投影。而显而易见的，这个投影后形成的点愈分散，能够保留更多的原始信息；而如果投影后生成的点几乎重合，就代表着信息几乎全部丢失。而从数学上，我们用以衡量这一点的分散程度的变量就是<strong>方差</strong>。</p>

<p>由此可见，经过主成分分析之后的数据实际上是用<strong>新的基</strong>来表示的向量。这也代表了，这些基已经不再具有原始数据维度的现实含义。所以，主成分分析之后的数据所采用的坐标系没有现实意义，仅作为数据分析的绘图指示。</p>

<p>关于线性代数的基变换，此处不再过多讨论。推荐，制作数学科普视频的大佬 3Blue1Brown ，在 Bilibili 和 YouTube 都有账号。</p>

<h4 id="代码实现">代码实现</h4>

<p>Python 的胜利！！！</p>

<p>事先载入的包</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">numpy.linalg</span> <span class="kn">import</span> <span class="n">eig</span>
</code></pre></div></div>

<p>类的编写</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PrincipalComponentAnalysis</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">matrix</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">original_matrix</span> <span class="o">=</span> <span class="n">matrix</span>
</code></pre></div></div>

<p>首先，我们计算出协方差矩阵，及 列向去均值的矩阵。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">mean_subtract_matrix</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">matrix</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">subtract</span><span class="p">(</span><span class="n">matrix</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">matrix</span><span class="p">,</span> <span class="n">axis</span><span class="p">))</span>


    <span class="k">def</span> <span class="nf">covariance_matrix_cal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">matrix</span><span class="p">):</span>
        <span class="n">mean_matrix</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean_subtract_matrix</span><span class="p">(</span><span class="n">matrix</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">mean_matrix</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">mean_matrix</span><span class="p">),</span> <span class="n">mean_matrix</span>
</code></pre></div></div>

<p>谱分解（此处直接采用 numpy.linalg 包中的 eig 函数）</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">spectral_decomposition</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">matrix</span><span class="p">):</span>
        <span class="n">eigen_values</span><span class="p">,</span> <span class="n">eigen_vectors</span> <span class="o">=</span> <span class="n">eig</span><span class="p">(</span><span class="n">matrix</span><span class="p">)</span> <span class="c1"># 这里返回的特征向量，满足 A = P*B*P^T, 其中矩阵中的特征向量为列向量
</span>        <span class="n">diagnal_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">eigen_values</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'Diagnal Matrix:'</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="n">diagnal_matrix</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">eigen_values</span><span class="p">,</span> <span class="n">eigen_vectors</span>
</code></pre></div></div>

<p>最后根据保留维数的需要，算出主成分就好了。（实际操作中，直接用 列向去均值的矩阵，而不用原始数据）</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">eigen_select</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reserved_dimension</span><span class="p">):</span> <span class="c1"># 保留对应较大特征值的特征向量
</span>        <span class="n">indice</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">eigen_values</span><span class="p">)</span> <span class="c1"># 注意这个函数返回的是排序后数组的索引，而不是数组，负号表示降序
</span>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">eigen_vectors</span><span class="p">[:,</span><span class="n">indice</span><span class="p">[:</span><span class="n">reserved_dimension</span><span class="p">]]</span>


    <span class="k">def</span> <span class="nf">principal_matrix_cal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dimension_coefficients</span><span class="p">):</span>
        <span class="n">principal_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mean_matrix</span><span class="p">,</span> <span class="n">dimension_coefficients</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">principal_matrix</span>
</code></pre></div></div>

<p>最后，所有函数凑成一个主函数</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reserved_dimension</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">covariance_matrix</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean_matrix</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">covariance_matrix_cal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">original_matrix</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eigen_values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eigen_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">spectral_decomposition</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">covariance_matrix</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Here is the spectral decompostion:"</span><span class="p">)</span>
        <span class="n">eigen_vectors_reserved</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eigen_select</span><span class="p">(</span><span class="n">reserved_dimension</span><span class="p">)</span>
        <span class="n">principal_matrix</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">principal_matrix_cal</span><span class="p">(</span><span class="n">eigen_vectors_reserved</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">principal_matrix</span>
</code></pre></div></div>

<p>当然，主成分分析这种事情，早就被人写成包了。我这里用的是 sklearn 中的 PCA</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>

<span class="n">original_matrix</span><span class="p">,</span> <span class="n">reserved_dimension</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mat</span><span class="p">(</span><span class="s">"1,2,3;2,1,3;2,4,6"</span><span class="p">),</span> <span class="mi">2</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">reserved_dimension</span><span class="p">)</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">original_matrix</span><span class="p">)</span>
</code></pre></div></div>

<p>PCA 的 fit_transform 方法可以直接算出对应矩阵的主成分矩阵。</p>

<p>根据我自己的多次测试，我自己写出的代码，跟 sklearn 中计算出的结果是一样的。</p>
:ET