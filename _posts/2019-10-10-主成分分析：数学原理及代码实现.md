---
layout:     post
title:      主成分分析：数学原理及代码实现
subtitle:Principal Components Analysis: Rationale and Realization
date:       2019-11-02
author:     Dundant
header-img: img/principal-component-analysis.jpg
catalog: true
tags:
    -math -数学 -概率 -金融 -finance
---

主成分分析，通常作为一种统计学的研究方法，在诸多领域都得到应用。而实际上，主成分分析的精髓则在于线性代数。

通常来看，对于原始数据，在做分析时，我们往往有以下两个诉求：

1. **各个维度的数据没有相关性**。各维度数据没有相关性意味着，根据数据推导出的因果关系不会受到其他维度数据的影响；同时，同一维度代表着这一维度的几乎全部信息，而不受到其他维度的推动。
2. **尽可能的去掉影响力小的维度**，**而保留重要的为数不多的维度**。主次分明的数据可以更好的被人类用以分析，从而得出主要的结果。

那么，主成分分析就是为了解决以上的两个诉求，而产生的数据预处理方法。主成分分析也有其自身的局限性，这一点本文不做讨论。

#### 数学原理

##### 一、具体阐释

原始数据通常不能直接作为分析的对象，于是我们需要用到某种方法找出新的数据。这就是主成分(Principal)。

要找出新的主成分，对原始信息降维，实际上是对原始信息中某些相关性高的维度合成一个。也就是说，把各维度的相关性从高到低排列，按某种方法将其中相关性过高的两个维度（或者三个、四个等）合成一个维度（加权平均，或者说线性组合）。

不过这样想，实际上有点多余，我们当然可以直接把所有的维度全部线性组合起来，只需要让某些系数等于零。为了保证之后主成分的大小不会有过大偏差，还会把这些系数的和不超过 1 。

这样就实现了数据的降维。

那么问题就来了：

1. 按哪一种方法针对相关的维度做线性组合？
2. 如何实现转换过后的数据各维度之间没有任何相关性？
3. 转换之后该用什么样的标准选取主成分？

也就是说，以上的三个问题就是这一数学模型的目标。

##### 二、数学模型

###### 模型建立

假设原始数据：
$$
X=
 \begin{bmatrix}
  x_1 & x_2 & ... & x_m
 \end{bmatrix}_{n\times m}
 \quad
 其中\;
 x_i
 \space是n维列向量 \quad
 \forall i \in \{1, 2,...,m\} \space
 \sum x_i=0
$$
而转换之后的数据矩阵长这个样子：
$$
F=
\begin{bmatrix}
f_1 & f_2 & ... & f_m
\end{bmatrix}_{n\times m}
\quad
其中\space 
f_i
\space 是n 维列向量
$$
那么又假定转换系数矩阵是

$$
U=
\begin{bmatrix}
u_1 & u_2 & ... & u_m
\end{bmatrix} _{m \times m}
\quad
其中 \space 
u_i
\space 是m维列向量 \quad
\sum u_i^2 = 1
$$
那么得到
$$
F=XU \\
其中有：\forall i \in \{1, 2, 3,...,m\} \quad f_i = Xu_i \space 也就是说，每一个主成分都是原始数据各维度的线性组合
$$
那么为了实现我们的目标，我们还需要一个计算维度间相关性的方法：**协方差矩阵**
$$
\begin{align}
X^{\rm T}X&=
\begin{bmatrix}
x_1 & x_2 & ... & x_m
\end{bmatrix} ^{\rm T}
\\
\begin{bmatrix}
x_1 & x_2 & ... & x_m
\end{bmatrix}
&=
\begin{bmatrix}
x_1 ^{\rm T} \\ x_2 ^{\rm T} \\ ... \\ x_m ^{\rm T}
\end{bmatrix}

\begin{bmatrix}
x_1 & x_2 & ... & x_m
\end{bmatrix} \\
&=
\begin{bmatrix}
x_1 ^{\rm T} x_1 & x_1 ^{\rm T} x_2 & ... & x_1 ^{\rm T} x_m \\
x_2 ^{\rm T} x_1 & x_2 ^{\rm T} x_2 & ... & x_2 ^{\rm T} x_m \\
...&...&...&...\\
x_m ^{\rm T} x_1 & x_m ^{\rm T} x_2 & ... & x_m ^{\rm T} x_m
\end{bmatrix}
\end{align}
$$
而可以发现：
$$
\begin{align}
x_i ^{\rm T} x_i &= Var(x_i)\\
 x_i^{\rm T} x_j &= x_j ^{\rm T} x_i =Cov(x_i,x_j)
\end{align}
$$
那么，**实际上 $X ^{\rm T} X$  就是针对 $X$ 中各列向量的协方差矩阵**。



###### 模型导出

现在回到我们对于 $F$ 的要求。各维度相关性为零，也就是说，$F$ 协方差矩阵是**对角矩阵**。
$$
\begin{align}
&F^{\rm T}F=(XU)^{\rm T} XU=U^{\rm T}X^{\rm T}X U \\
&Let \; X^{\rm T}X=C,\; F^{\rm T}F = D, \; then \; D=U^{\rm T}CU \\
&C=(U^{\rm T})^{-1}DU^{-1}
\end{align}
$$
$C$ 是实对称矩阵，$D$ 是对角矩阵，而 $U$ 是也是方阵。眼尖的同学已经发现了，只要我们把矩阵 $C$ 进行**谱分解**，就可以得到这样的结果。

线性代数比较好的同学当然也知道：**正交矩阵的逆矩阵等于其转置矩阵**。（这也就是我们在假定中针对U做出如此限定的原因） $U$ 是正交矩阵，也就是 $C$ 的特征矩阵 $P$ 。于是，我们的结论就是 原始数据的协方差矩阵的特征矩阵就是这里的降维系数矩阵。



###### 主成分的选取

至此，问题 1 和问题 2 都得到了解决。还剩下了问题 3 。答案很简单，我们是根据 f 的方差大小决定去留的。方差更大的主成分更值得保留。

那么问题又来了，为什么会是这样？更大的方差到底意味着什么？

不妨这样思考：方差如果太小会怎么样？更小的方差意味着这一维度的数据将会更加集中，而信息量将会减少，而方差为零则更加说明，这一主成分几乎不起任何作用。

那么可以根据主成分的协方差矩阵，在矩阵 $F$ 选取方差较大的**一列**作为，主成分分析的结果。保留的维数根据分析的需求而定，当然也可以随意指定。



###### 基变换和线性空间

从线性代数的角度来说，生成主成分实际上是一次**基变换**。我们可以把现有主成分称为原始数据在新的基上的投影。而显而易见的，这个投影后形成的点愈分散，能够保留更多的原始信息；而如果投影后生成的点几乎重合，就代表着信息几乎全部丢失。而从数学上，我们用以衡量这一点的分散程度的变量就是**方差**。

由此可见，经过主成分分析之后的数据实际上是用**新的基**来表示的向量。这也代表了，这些基已经不再具有原始数据维度的现实含义。所以，主成分分析之后的数据所采用的坐标系没有现实意义，仅作为数据分析的绘图指示。

关于线性代数的基变换，此处不再过多讨论。推荐，制作数学科普视频的大佬 3Blue1Brown ，在 Bilibili 和 YouTube 都有账号。



#### 代码实现

Python 的胜利！！！

事先载入的包

```python
import numpy as np
from numpy.linalg import eig
```

类的编写

```python
class PrincipalComponentAnalysis(object):
    def __init__(self, matrix):
        self.original_matrix = matrix
```

首先，我们计算出协方差矩阵，及 列向去均值的矩阵。

```python
    def mean_subtract_matrix(self, matrix, axis=0):
        return np.subtract(matrix, np.mean(matrix, axis))


    def covariance_matrix_cal(self, matrix):
        mean_matrix = self.mean_subtract_matrix(matrix)
        return np.dot(mean_matrix.T, mean_matrix), mean_matrix
```

谱分解（此处直接采用 numpy.linalg 包中的 eig 函数）

```python
    def spectral_decomposition(self, matrix):
        eigen_values, eigen_vectors = eig(matrix) # 这里返回的特征向量，满足 A = P*B*P^T, 其中矩阵中的特征向量为列向量
        diagnal_matrix = np.diag(eigen_values)
        print('Diagnal Matrix:')
        print(diagnal_matrix)
        return eigen_values, eigen_vectors
```

最后根据保留维数的需要，算出主成分就好了。（实际操作中，直接用 列向去均值的矩阵，而不用原始数据）

```python
    def eigen_select(self, reserved_dimension): # 保留对应较大特征值的特征向量
        indice = np.argsort(-self.eigen_values) # 注意这个函数返回的是排序后数组的索引，而不是数组，负号表示降序
        return self.eigen_vectors[:,indice[:reserved_dimension]]


    def principal_matrix_cal(self, dimension_coefficients):
        principal_matrix = np.dot(self.mean_matrix, dimension_coefficients)
        return principal_matrix
```

最后，所有函数凑成一个主函数

```python
    def main(self, reserved_dimension):
        self.covariance_matrix, self.mean_matrix = self.covariance_matrix_cal(self.original_matrix)
        self.eigen_values, self.eigen_vectors = self.spectral_decomposition(self.covariance_matrix)
        print("Here is the spectral decompostion:")
        eigen_vectors_reserved = self.eigen_select(reserved_dimension)
        principal_matrix = self.principal_matrix_cal(eigen_vectors_reserved)
        return principal_matrix
```



当然，主成分分析这种事情，早就被人写成包了。我这里用的是 sklearn 中的 PCA

```python
import numpy as np
from sklearn.decomposition import PCA

original_matrix, reserved_dimension = np.mat("1,2,3;2,1,3;2,4,6"), 2
pca = PCA(n_components=reserved_dimension)
pca.fit_transform(original_matrix)
```

PCA 的 fit_transform 方法可以直接算出对应矩阵的主成分矩阵。



根据我自己的多次测试，我自己写出的代码，跟 sklearn 中计算出的结果是一样的。 
