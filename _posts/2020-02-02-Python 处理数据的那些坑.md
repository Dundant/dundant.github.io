---
layout:     post
title:      Python 处理数据的哪些坑
subtitle:   Pitfalls in Data Analysis Using Python
date:       2020-02-02
author:     Dundant
header-img: img/007-Python-data-preprocess.png
catalog:    true
tags:
    math Python data_analysis
---
> 本文作为自己处理一些数据的经验之谈，当然也是基于我自身痛苦经历的分享。写此文既是为了给读者一个能够辨识规避个中错误的方法，姑且称之为一份轻指南，另外写此文也是为了让我以后不再反相同的错误，共同取得进步，与诸君共勉。

## 起因
这篇文章的产出主要是因为2019年我的两份课程作业，关键是写这两份作业的过程实在是太痛苦了。**痛苦之处就在于数据的处理**。当然，之所以这么痛苦，是因为数据的结构不是标准化的，以至于有太多的意想不到的错误。

三份数据分别是：源自[@王晓磊同学](http://weibo.com/xiaoleiwang)[网站](https://beijingair.sinaapp.com/)的中国城市空气质量历史数据、中国气象监测站的历史数据和源自Python金融数据库的[Tushare](https://tushare.pro/)。

此处不会提及这两份作业的具体内容。

## 单文件表格处理
如果只是一个文件，而且格式很完备，就比如最常见的 csv 文件，也即 Comma Seperated Values。
 - 如果每一行的列数不一致，就必须写脚本，改制文件
 - 如果每一行的列数一致，可直接使用 read_csv 读取

#### 表格格式不完备
这种的问题的处理视具体情况而定，个中情况不一而足。不过解决办法大概只有两个，要么手动修改文本文件，要么写脚本修改文本文件。

**可能用到的Python方法**

写脚本修改文本文件需要用到Python的字符串格式化。可能会需要用到下列字符串方法：
 - format 字符串格式化
 - join 组合可迭代对象
 - split 切分字符串
 - strip 除去字符串两端的字符（含lstrip、rstrip）

还有一些文件对象的方法：
 - read 加载全部文件内容进入内存，返回字符串
 - readline 加载文件指针的一行进入内存，返回字符串
 - readlines 加载文件全部内容进入内存，返回元素是文件每一行的列表
 - write 写入文件
 - writeline 字符写入文件并换行
 - writelines 文件写入多行

以及一些正则表达式的方法：
 - re.sub 按规则替换指定字符
 - re.match 按规则匹配指定字符

当然还有一些正则表达式的用法：
 - 空白符
 - 数字
 - 字母
 - 匹配空或者长度大于或等于1的任意字符
 - 匹配1个或者长度大于1的任意字符
 - () 表示 match 中的分组

#### 文件体积过大
过大的文件体积（比如超过1个 G ）会导致 Pandas 无法载入文件到计算机内存。这样的后果就是死机。比如一个１G 大小的文件，对于一个8G内存的家用机就会过载的。其原因在于，Pandas 读取表格文件之后的 DataFrame 会占用超过文件本身的大量内存。

**可行的解决办法**

 - 利用 Pandas 的分块读取文件的功能，即把文件分成一个个 Chunk，并产生一个个的 DataFrame。暂时还不清除这种做法的缺点。

 - 利用 [Vaex](https://vaex.io/) 库。Vaex 利用了 hdf5 格式，Python 使用这个库能够在家用计算机上读取超大型表格，而且运算速度很快。Vaex 表格对象的具体的使用方法与一般的 DataFrame 的用法不尽相同。Vaex 可直接读取 csv、hdf5格式。

 - 利用 Google 提供的在线 Jupyter Colab 环境直接读取大文件。这个环境的内存可达12G，如果发生由于内存不足而产生的死机问题，Google 还会提供一个内存高达 25G 的基于 linux 的 jupyter 环境。

![Googel Colab](https://i.loli.net/2020/02/05/YRdu45lSKeHrWi2.png)

在这些解决办法中，最好的还是直接用 Vaex，或者把 Colab 和 Vaex 相结合。其原因在于 Vaex 有着很快的运算速度，并且有着针对大型数据库有着的优化。

**不过 Vaex 的安装可能会存在某些问题**。根据 Vaex 的官方文档，Vaex 适合在 virtualenv 虚拟环境安装使用，而在系统范围或者用户范围安装都是不推荐的。此外，Vaex 安装包括多个组件，根据我的实际使用情况来看，在 linux 系统上的虚拟环境直接使用 pip install Vaex 会产生问题，就是 pyarrow 的安装会出现问题。具体解决办法未知，未在 windows 上尝试。

![安装问题截图](https://sm.ms/delete/LtN7YFZMmGHduaoc15KByxe4Rk)

不过我的解决办法是：**直接用 pip install Vaex-hdf5 语句安装**，用这个解决办法，可直接 import Vaex，使用过程中暂时也未发现问题。暂时未发现这个问题是如何引起的，官方文档也未作说明。Plus，我的系统是 manjaro，Python 版本是3.8.1。（在3.7、3.6.5版本上也都发现相同的问题）

![系统信息](https://ae01.alicdn.com/kf/Ha992815b53424ee28048e605c3026b9bE.png)

#### 空值
如果不去掉空值，在模型的运算过程中，有可能会报错，即使不报错也有可能产生坏的结果。去掉空值的方法有两种：fillna 和 dropna，这分别代表的是填空值和去空值，二者都是DataFrame的方法。

**可行的解决办法**

fillna 填空值的方法有三种，一种是填浮点0，一种是从上到下，另外一种是从下到上，对应的参数是method，值分别是{'backfill', 'bfill', 'pad', 'ffill', None}．默认是None，也即全部填0．backfill等价于bfill，pad等价于fill．前者是用下一个数填空值，后者是用上一个数填空值．注意：连续空值会用非空值填．默认是填充0．

当然是可以自己编写填空值的方法的．这里需要利用的是numpy库中判断空值的函数isnan．同时可能需要用到DataFrame的apply方法．

dropna 去空值的方法有两种，一种是去掉行，另外一种是去掉列．当然一般都是去掉行．对应axis=0．默认是去掉行

另外需要注意的一点是，inplace参数．此参数为True，则原始DataFrame文件直接修改，返回值为None；为False，则原始DataFrame文件无修改，返回值为处理空值后的DataFrame．

**可能用到的Python方法**

Pandas库的一些函数：
 - read_csv 读取csv表格，可指定列标签和行标签(header and index_column)
 - read_excel
　- to_csv
 - concat 拼接多个DataFrame，多个DataFrame用列表
 - merge　联接两个dataframe

DataFrame的一些方法：
 - iterrows 遍历行
 - iteritems 遍历列
 - dropna 去空值
 - fillna 填空值
 - apply 逐行或列运算函数
 - group 按种类分组，常用于求各组平均或者计数
 - cut 按各区间分组，相当于数据列的group
 - append 添加一行或者一列，有inplace参数

#### 重复行
如果不认真处理重复行，重复行在某些情况下是会产生一些很难发现的错误的。

通常在 sql 数据库中，index 或者 id 是不可以重复的。在 DataFrame 中 row index 也是不可以重复的。但是一般在读取文件表格的时候，我们并不会使用表格中原本就有的 column index 作为 index 或者 id，也即是主键。

这样的处理虽然会带来一些好处，例如在不需要了解表格全貌的时候，就可以读取 DataFrame，不过问题就在于，这样就可能产生一些重复行；而在另外一些更狡猾的情况下，重复行的产生就变得更为平常了。就比如，在将多个 DataFrame 纵向(也即 axis=0 )组装的时候，通常我们会选择 ignore_index=True，这就极容易产生重复行的问题了。

好在应对办法也很简单，

至于重复行会引起的问题，除了会引起模型的结果不准确以外。另外的问题就是**在求笛卡尔集的时候，也即 merge 这个函数，会产生大量重复行，乃至于重复多次会致使内存爆掉**。

#### 二维表
讨论这个问题之前，必须明晰一下什么叫二维表，什么叫一维表。

二维表与一维表的区别就在于：一维表的每一行都是一个观测值，或者说是一个单独的对象，显著特征就是每一列都是被观测对象的一个维度的属性。而二维表则不服从这一个规律，二维表的每一个格子才是一个观测值，那么这样的话，不适合计量模型分析。

解决办法也有。

## 多个表格文件的处理
比如一个数据按如下的方式组织：

也即，其中的每一个文件代表了一个小的表格，而文件夹则表示了一种分类－－这通常是按时间。

由于表格分散在多个文件中，而且在多数情况下，不能够直接阅读表格，所以是没有办法产生一个对数据的全貌的。在这种情况下，数据的说明，就显得极为重要。不过由于数据的整理方可能在形成数据中发生人工错误，或者由于数据本身源于某种自动化作业，本身缺少人工审查，因此数据也有可能会产生不可预料的错误，又或者由于技术变革或者社会投入增加、乃至法律国家标准的改变导致的随着时间的发展，数据本身会产生变化－－例如新的空气检测标准使得原有的空气污染指数(API)被现行的空气质量指数(AQI)，当然这些都不重要，最好的是直接提供空气中各项污染物的浓度，而不是一个意义不明的空气质量指数。当然新的空气检测标准真正的用处是增加的污染物的观测项目数。

又比如，大气气象检测站点，不可能在一开始就全部建立，而是必须一个一个站点的建立，这就导致了，后来建立的站点，没有之前的数据。对于研究无用。

除了时间维度上的前后不一致，还有一个问题可能是，某些表格文件实际上是完全错误的。比如数据都是用网页爬虫爬取，那么就有可能有几个文件就不是格式化的文本，而是html错误页面。所以在写代码的时候，就必须注意到这一点，也即是 try except 结构。

值得注意的是，再使用try except的时候，尽量不要使用不带 Exception 的 except 语句。因为这样做可能会让程序异常，产生不符合的结果，如果能知道结果不是良好的，那也不是问题。关键就在于如果结果不符合预期，同时又发现不了。那就是个大问题了。所以尽量不要使用不带 Exception 的 except 语句。也就是说，所有可预期的的异常情况都需要在代码中体现出来。异常是无限的，同时也是呈现显著单峰分布的，所以必须要使用有限的异常情况对应处理绝大多数的异常。

当然，多表格的处理，同时也会出现上述单表格文件的问题。

## 总结
数据处理可能出现的问题：
 - 空值
 - 重复行，尤其是拼接 DataFrame 的过程中
 - 二维表到一维表的转换
 - 多表格拼接的字段，异常处理


最后修改于2月5日
