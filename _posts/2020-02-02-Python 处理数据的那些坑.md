---
layout:     post
title:      Python 处理数据的哪些坑
subtitle:   Pitfalls in Data Analysis Using Python
date:       2020-02-02
author:     Dundant
header-img: img/007-Python-data-preprocess.png
catalog:    true
tags:
    math Python data_analysis
---
> 本文作为自己处理一些数据的经验之谈，当然也是基于我自身痛苦经历的分享。写此文既是为了给读者一个能够辨识规避个中错误的方法，姑且称之为一份轻指南，另外写此文也是为了让我以后不再反相同的错误，共同取得进步，与诸君共勉。

## 起因
这篇文章的产出主要是因为2019年我的两份课程作业，关键是写这两份作业的过程实在是太痛苦了。**痛苦之处就在于数据的处理**。当然，之所以这么痛苦，是因为数据的结构不是标准化的，以至于有太多的意想不到的错误。

三份数据分别是：源自[@王晓磊同学](http://weibo.com/xiaoleiwang)[网站](https://beijingair.sinaapp.com/)的中国城市空气质量历史数据、中国气象监测站的历史数据和源自Python金融数据库的[Tushare](https://tushare.pro/)。

此处不会提及这两份作业的具体内容。

## 单文件表格处理
如果只是一个文件，而且格式很完备，就比如最常见的 csv 文件，也即 Comma Seperated Values。
 - 如果每一行的列数不一致，就必须写脚本，改制文件
 - 如果每一行的列数一致，可直接使用 read_csv 读取

#### 表格格式不完备
这种的问题的处理视具体情况而定，个中情况不一而足。不过解决办法大概只有两个，要么手动修改文本文件，要么写脚本修改文本文件。

**可能用到的Python方法**

写脚本修改文本文件需要用到Python的字符串格式化。可能会需要用到下列字符串方法：
 - format 字符串格式化
 - join 组合可迭代对象
 - split 切分字符串
 - strip 除去字符串两端的字符（含lstrip、rstrip）

还有一些文件对象的方法：
 - read 加载全部文件内容进入内存，返回字符串
 - readline 加载文件指针的一行进入内存，返回字符串
 - readlines 加载文件全部内容进入内存，返回元素是文件每一行的列表
 - write 写入文件
 - writeline 字符写入文件并换行
 - writelines 文件写入多行

以及一些正则表达式的方法：
 - re.sub 按规则替换指定字符
 - re.match 按规则匹配指定字符

当然还有一些正则表达式的用法：
 - 空白符
 - 数字
 - 字母
 - 匹配空或者长度大于或等于1的任意字符
 - 匹配1个或者长度大于1的任意字符
 - () 表示 match 中的分组

#### 文件体积过大
过大的文件体积（比如超过1个 G ）会导致 Pandas 无法载入文件到计算机内存。这样的后果就是死机。比如一个１G 大小的文件，对于一个8G内存的家用机就会过载的。其原因在于，Pandas 读取表格文件之后的 DataFrame 会占用超过文件本身的大量内存。

**可行的解决办法**

 - 利用 Pandas 的分块读取文件的功能，即把文件分成一个个 Chunk，并产生一个个的 DataFrame。暂时还不清除这种做法的缺点。

 - 利用 [Vaex](https://vaex.io/) 库。Vaex 利用了 hdf5 格式，Python 使用这个库能够在家用计算机上读取超大型表格，而且运算速度很快。Vaex 表格对象的具体的使用方法与一般的 DataFrame 的用法不尽相同。Vaex 可直接读取 csv、hdf5格式。

 - 利用 Google 提供的在线 Jupyter Colab 环境直接读取大文件。这个环境的内存可达12G，如果发生由于内存不足而产生的死机问题，Google 还会提供一个内存高达 25G 的基于 linux 的 jupyter 环境。

![Googel Colab](https://i.loli.net/2020/02/05/YRdu45lSKeHrWi2.png)

在这些解决办法中，最好的还是直接用 Vaex，或者把 Colab 和 Vaex 相结合。其原因在于 Vaex 有着很快的运算速度，并且有着针对大型数据库有着的优化。

**不过 Vaex 的安装可能会存在某些问题**。根据 Vaex 的官方文档，Vaex 适合在 virtualenv 虚拟环境安装使用，而在系统范围或者用户范围安装都是不推荐的。此外，Vaex 安装包括多个组件，根据我的实际使用情况来看，在 linux 系统上的虚拟环境直接使用 pip install Vaex 会产生问题，就是 pyarrow 的安装会出现问题。具体解决办法未知，未在 windows 上尝试。

![安装问题截图](https://sm.ms/image/nCMwS1u2chBoHxZ)

不过我的解决办法是：**直接用 pip install Vaex-hdf5 语句安装**，用这个解决办法，可直接 import Vaex，使用过程中暂时也未发现问题。暂时未发现这个问题是如何引起的，官方文档也未作说明。Plus，我的系统是 manjaro，Python 版本是3.8.1。（在3.7、3.6.5版本上也都发现相同的问题）

![系统信息](https://ae01.alicdn.com/kf/Ha992815b53424ee28048e605c3026b9bE.png)

#### 空值
如果不去掉空值，在模型的运算过程中，有可能会报错，即使不报错也有可能产生坏的结果。去掉空值的方法有两种：fillna 和 dropna，这分别代表的是填空值和去空值，二者都是DataFrame的方法。

**可行的解决办法**

fillna 填空值的方法有三种，一种是填浮点0，一种是从上到下，另外一种是从下到上，对应的参数是method，值分别是{'backfill', 'bfill', 'pad', 'ffill', None}．默认是None，也即全部填0．backfill等价于bfill，pad等价于fill．前者是用下一个数填空值，后者是用上一个数填空值．注意：连续空值会用非空值填．默认是填充0．

当然是可以自己编写填空值的方法的．这里需要利用的是numpy库中判断空值的函数isnan．同时可能需要用到DataFrame的apply方法．

dropna 去空值的方法有两种，一种是去掉行，另外一种是去掉列．当然一般都是去掉行．对应axis=0．默认是去掉行

另外需要注意的一点是，inplace参数．此参数为True，则原始DataFrame文件直接修改，返回值为None；为False，则原始DataFrame文件无修改，返回值为处理空值后的DataFrame．

**可能用到的Python方法**

Pandas库的一些函数：
 - read_csv 读取csv表格，可指定列标签和行标签(header and index_column)
 - read_excel
 - concat 拼接多个DataFrame，多个DataFrame用列表
 - merge　联接两个dataframe

DataFrame的一些方法：
 - iterrows 遍历行
 - iteritems 遍历列
 - dropna 去空值
 - fillna 填空值
 - apply 逐行或列运算函数
 - group 按种类分组，常用于求各组平均或者计数
 - cut 按各区间分组，相当于数据列的group
 - append 添加一行或者一列，有inplace参数
 - to_csv dataframe写入csv文件

#### 重复行
如果不认真处理重复行，重复行在某些情况下是会产生一些很难发现的错误的。

在 DataFrame 中行标签是不可以重复的。但是一般在读取文件表格的时候，并不会使用表格中的字段作为行标签，而是采用默认的序数标签。如果要使用文件中的字段作为行标签可以在read_csv中指定index_col。这就造成了，读取的文件中可能出现重复行。

另外一种情况，DataFrame拼接的过程中，可能会产生重复行。因为纵向拼接通常会选择ignore_index=True。

至于重复行会引起的问题，除了会引起模型的结果不准确以外。另外的问题就是**在求笛卡尔集的时候，也即 merge 这个函数，会产生大量重复行，乃至于重复多次会致使内存爆掉**。

**解决办法**

使用dataframe的drop_dupilicates方法。去掉所有的重复行。

#### 二维表
讨论这个问题之前，必须明晰一下什么叫二维表，什么叫一维表。

二维表与一维表的区别就在于：一维表的每一行都是一个观测值，或者说是一个单独的对象，显著特征就是每一列都是被观测对象的一个维度的属性。而二维表则不服从这一个规律，二维表的每一个格子才是一个观测值，那么这样的话，不适合计量模型分析。

**解决办法**

可参考[这篇文章](https://blog.csdn.net/qq_41080850/article/details/86294173)

## 多个表格文件的处理
比如一个数据按如下的方式组织：

```Python
城市空气质量
|---城市_20140513-20141231
|   |---china_cities_20140513.csv
|   |---china_cities_20140514.csv
|   |---china_cities_20140515.csv
|   |---......
|   |---china_cities_20141231.csv
|---城市_20150101-20151231
|   |---china_cities_20150101.csv
|   |---......
|---城市_20160101-20161231
|   |---china_cities_20160101.csv
|   |---......
|---城市_20170101-20171231
|   |---china_cities_20170101.csv
|   |---......
|---城市_20180101-20181231
|   |---china_cities_20180101.csv
|   |---......
|---城市_20190101-20191214
|   |---china_cities_20190101.csv
|   |---......
```
其中的每一个文件代表了一天中，全国大城市的空气质量（每隔一小时发布），而文件夹则是按年分类的。

由于表格分散在多个文件中，直接阅读表格，是没有办法产生一个对数据的全貌的。在这种情况下。所以我们只能靠数据的说明来产生对数据的理解。不过这样由于某些原因数据也会出现错误，比如人工错误，或者自动化操作的错误。为了避免这些错误影响到模型的结果，我们需要识别这些错误，那么就是利用try except。

值得注意的是，再使用try except的时候，尽量不要使用不带 Exception 的 except 语句。因为这样做可能会让程序异常，产生不符合的结果，如果能知道结果不是良好的，那也不是问题。关键就在于如果结果不符合预期，同时又发现不了。那就是个大问题了。所以尽量不要使用不带 Exception 的 except 语句。也就是说，所有可预期的的异常情况都需要在代码中体现出来。异常是无限的，同时也是呈现显著单峰分布的，所以必须要使用有限的异常情况对应处理绝大多数的异常。

## 总结
数据处理可能出现的问题：
 - 空值
 - 重复行，尤其是拼接 DataFrame 的过程中
 - 二维表到一维表的转换
 - 多表格拼接的字段，异常处理


最后修改于2月5日
