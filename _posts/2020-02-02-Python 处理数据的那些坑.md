---
layout:     post
title:      python 处理数据的哪些坑
subtitle:   Pitfalls in Data Analysis Using Python
date:       2020-02-02
author:     Dundant
header-img: img/007-python-data-preprocess.png
catalog:    true
tags:
    math python data_analysis
---
> 本文作为自己处理一些数据的经验之谈，当然也是基于我自身痛苦经历的分享。写此文既是为了给读者一个能够辨识规避个中错误的方法，姑且称之为一份轻指南，另外写此文也是为了让我以后不再反相同的错误，共同取得进步，与诸君共勉。

## 起因
文中所提及的案例，均是源自2019年我的两份课程作业的，此处不会提及这两份作业的具体内容。

## 单文件表格处理
如果只是一个文件，而且格式很完备，就比如最常见的 csv 文件，也即 Comma Seperated Values。
 - 如果每一行的列数不一致，就必须写脚本，改制文件
 - 如果每一行的列数一致，可直接使用 pandas.read_csv 读取

#### 表格格式不完备


#### 文件体积过大，超过计算机内存
过大的文件体积（比如超过1个 G ）会导致 pandas 无法载入文件到计算机内存。实际上，计算机通常会死机。比如一个１G 大小的文件，对于一个8G内存的家用机就会过载的。其原因在于，pandas 读取表格文件之后的 dataframe 会占用超过文件本身的大量内存。

   **解决办法**
 - 利用 pandas 的分快读取文件的功能，即把文件分成一个个 chunk，并产生一个个的 dataframe。暂时还不清除这种做法的缺点。

 - 利用 vaex 库。vaex 利用了 hdf5　格式，Python 使用这个库能够在家用计算机上读取超大型表格，而且运算速度很快。vaex 表格对象的具体的使用方法与一般的 dataframe 的用法不尽相同。vaex 可直接读取 csv、hdf5格式。

 - 利用 Google 提供的在线 Jupyter Colab 环境直接读取大文件。这个环境的内存可达12G，如果发生由于内存不足而产生的死机问题，Google 还会提供一个内存高达 25G 的基于 linux 的 jupyter 环境。

在这些解决办法中，最好的还是直接用 Vaex，或者把 Colab 和 Vaex 相结合。其原因在于 Vaex 有着很快的运算速度，并且有着针对大型数据库有着的优化。

**不过 Vaex 的安装可能会存在某些问题**。根据 Vaex 的官方文档，Vaex 适合在 virtualenv 虚拟环境安装使用，而在系统范围或者用户范围安装都是不推荐的。此外，Vaex 安装包括多个组件，根据我的实际使用情况来看，在 linux 系统上的虚拟环境直接使用 pip install vaex 会产生问题，就是 vaex-hdf5 的安装会出现问题。

不过我的解决办法是：**直接用 pip install vaex-hdf5 语句安装**，用这个解决办法，可直接 import vaex，使用过程中暂时也未发现问题。暂时未发现这个问题是如何引起的，官方文档也未作说明。Plus，我的系统是　manjaro，Python 版本是3.8.1。（在3.7、3.6.5版本上也都发现相同的问题）

![系统信息](https://ae01.alicdn.com/kf/Ha992815b53424ee28048e605c3026b9bE.png)

#### 空值
有些模型如果不去掉空值，就会报错。方法有两种：fillna 和　dropna，就如同英文名一样，这份别代表的是填空值和去空值。填空值的方法有两种，分别是用前一天或者后一天的值填空。

#### 重复行
如果不认真处理重复行，重复行在某些情况下是会产生一些很难发现的错误的。

通常在 sql 数据库中，index 或者 id 是不可以重复的。在 dataframe 中 row index 也是不可以重复的。但是一般在读取文件表格的时候，我们并不会使用表格中原本就有的　column index 作为　index 或者 id，也即是主键。

这样的处理虽然会带来一些好处，例如在不需要了解表格全貌的时候，就可以读取 dataframe，不过问题就在于，这样就可能产生一些重复行；而在另外一些更狡猾的情况下，重复行的产生就变得更为平常了。就比如，在将多个 dataframe 纵向(也即 axis=0 )组装的时候，通常我们会选择　ignore_index=True，这就极容易产生重复行的问题了。

好在应对办法也很简单，

至于重复行会引起的问题，除了会引起模型的结果不准确以外。另外的问题就是**在求笛卡尔集的时候，也即 merge 这个函数，会产生大量重复行，乃至于重复多次会致使内存爆掉**。

#### 二维表
讨论这个问题之前，必须明晰一下什么叫二维表，什么叫一维表。

二维表与一维表的区别就在于：一维表的每一行都是一个观测值，或者说是一个单独的对象，显著特征就是每一列都是被观测对象的一个维度的属性。而二维表则不服从这一个规律，二维表的每一个格子才是一个观测值，那么这样的话，不适合计量模型分析。

解决办法也有。

## 多个表格文件的处理
比如一个数据按如下的方式组织：

也即，其中的每一个文件代表了一个小的表格，而文件夹则表示了一种分类－－这通常是按时间。

由于表格分散在多个文件中，而且在多数情况下，不能够直接阅读表格，所以是没有办法产生一个对数据的全貌的。在这种情况下，数据的说明，就显得极为重要。不过由于数据的整理方可能在形成数据中发生人工错误，或者由于数据本身源于某种自动化作业，本身缺少人工审查，因此数据也有可能会产生不可预料的错误，又或者由于技术变革或者社会投入增加、乃至法律国家标准的改变导致的随着时间的发展，数据本身会产生变化－－例如新的空气检测标准使得原有的空气污染指数(API)被现行的空气质量指数(AQI)，当然这些都不重要，最好的是直接提供空气中各项污染物的浓度，而不是一个意义不明的空气质量指数。当然新的空气检测标准真正的用处是增加的污染物的观测项目数。

又比如，大气气象检测站点，不可能在一开始就全部建立，而是必须一个一个站点的建立，这就导致了，后来建立的站点，没有之前的数据。对于研究无用。

除了时间维度上的前后不一致，还有一个问题可能是，某些表格文件实际上是完全错误的。比如数据都是用网页爬虫爬取，那么就有可能有几个文件就不是格式化的文本，而是html错误页面。所以在写代码的时候，就必须注意到这一点，也即是　try except 结构。

值得注意的是，再使用try except的时候，尽量不要使用不带 Exception　的　except　语句。因为这样做可能会让程序异常，产生不符合的结果，如果能知道结果不是良好的，那也不是问题。关键就在于如果结果不符合预期，同时又发现不了。那就是个大问题了。所以尽量不要使用不带 Exception 的　except　语句。也就是说，所有可预期的的异常情况都需要在代码中体现出来。异常是无限的，同时也是呈现显著单峰分布的，所以必须要使用有限的异常情况对应处理绝大多数的异常。

当然，多表格的处理，同时也会出现上述单表格文件的问题。

## 总结
数据处理可能出现的问题：
 - 空值
 - 重复行，尤其是拼接 dataframe 的过程中
 - 二维表到一维表的转换
 - 多表格拼接的字段，异常处理
